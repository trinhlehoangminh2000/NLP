{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3132,"status":"ok","timestamp":1665946365772,"user":{"displayName":"Lê Hoàng Minh Trịnh","userId":"12567939653044008206"},"user_tz":-120},"id":"824FC_aU9HRV","outputId":"e0aa417c-47dd-4a50-894c-f0fa6dd2454b"},"outputs":[],"source":["import csv\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","\n","# packages from torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import re\n","\n","from io import StringIO\n","import requests"]},{"cell_type":"markdown","metadata":{"id":"UFaz2KrIdDgA"},"source":["# Load Data\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"q0gVzd0_9rlf"},"outputs":[],"source":["def load_scifi():\n","    with open('./dataset/scifi.txt') as f:\n","        lines = f.readlines()\n","    text = lines[0]\n","    return text \n","def load_tripadvisor():\n","    # Load trip advisor\n","    df = pd.read_csv('./dataset/tripadvisor_hotel_reviews.csv')\n","    df.columns = ['Review','Rating']\n","    text = ''\n","    for review in df['Review']:\n","        text+=review\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trip = load_tripadvisor()\n","scifi = load_scifi()"]},{"cell_type":"markdown","metadata":{"id":"hvnfXuPxA-cC"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["### Todo \n","- Remove special characters and punctuation\n","- Lower case\n","- Correct spelling\n","- Emojies\n","- Chat word treatment\n","- Remove URLs (if needed)\n","- Remove HTML tags (if needed)\n","- Remove rare words because we dont have enough statistic\n","- Answer if we need to implement:\n","    - Remove stopword? (not needed if we choose a large window size). Because stop words are frequent and does not have much meaning so think about increasing the window size so that it is not so relevant anymore. If we choose a smaller window size, it makes sense to remove predicting context using stop word as the target word in CBOW)\n","    - Stemming is not needed."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Implement code here\n","# Example from tutorial notebook: \"basic-text-preprocessing-template-nlp.ipynb\""]},{"cell_type":"markdown","metadata":{},"source":["# Build CBOW model"]},{"cell_type":"markdown","metadata":{},"source":["Create class CBOW. Take parameters as vocab size and embedding dimensions as input to contruct layers"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["class CBOW(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","        \n","        # Out (context_size*2) x embedding_dimensions\n","        # But will take sum in forward \n","        # => Out 1 x embedding_dimensions\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Out 1 x 128\n","        self.linear1 = nn.Linear(embedding_dim, 128)\n","        \n","        # Out 1 x 128\n","        # Activation function Relu\n","        self.act1 = nn.ReLU()\n","\n","        # Out 1 x vocab_size\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","        # Out 1 x vocab_size\n","        self.act2 = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, inputs):\n","        # Start of layer 1\n","        l1 = self.embeddings(inputs)\n","        # l1 is of shape (context_size*2) x embedding_dimensions\n","        # sum them up an transform them to 1 x embedding_dimension\n","        l1 = sum(l1).view(1,-1)\n","        l1 = self.linear1(l1)\n","        l1 = self.act1(l1)\n","        \n","        # Start of layer 2, probability is the output of log softmax\n","        l2 = self.linear2(l1)\n","        prob = self.act2(l2)\n","        return prob\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)"]},{"cell_type":"markdown","metadata":{},"source":["Define vocab, etc."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["vocab = set()\n","vocab_size = ...\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","data = []\n","for i in range(2, len(raw_text) - 2):\n","    context = [raw_text[i - 2], raw_text[i - 1],\n","               raw_text[i + 1], raw_text[i + 2]]\n","    target = raw_text[i]\n","    data.append((context, target))"]},{"cell_type":"markdown","metadata":{},"source":["Define parmeters, network and loss function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right\n","EMBEDDING_DIM = 100 \n","\n","losses = []\n","# choose NLLLoss we want to minize the negative log of softmax\n","loss_function = nn.NLLLoss()\n","\n","model = CBOW(vocab_size, EMBEDDING_DIM)\n","\n","# Choose optimizer as SGD\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["Trainning loop"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["for epoch in range(100):\n","    total_loss=0\n","    for context, target in data:\n","        # create vector representaion of the context words (by its index from vocab)\n","        context_vector = make_context_vector(context, word_to_ix)\n","        log_probs = model(context_vector)\n","        target_vector = torch.tensor([word_to_ix[target]], dtype=torch.long)\n","        loss = loss_function(log_probs,target_vector)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss+=loss.item()\n","    losses.append(total_loss)"]},{"cell_type":"markdown","metadata":{},"source":["# Result\n","\n","Nearest neighbor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_closest_word(word, topn=5):\n","    word_distance = []\n","    emb = net.embeddings_target\n","    pdist = nn.PairwiseDistance()\n","    i = word_to_index[word]\n","    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n","    v_i = emb(lookup_tensor_i)\n","    for j in range(len(vocabulary)):\n","        if j != i:\n","            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n","            v_j = emb(lookup_tensor_j)\n","            2\n","            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n","    word_distance.sort(key=lambda x: x[1])\n","    return word_distance[:topn]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOf+J+HVMCcRAv+X14MdZh3","collapsed_sections":["UFaz2KrIdDgA","jy0KBRDydHYE","BIgN-4nqMyjB","elvPlKkCdRiO","uLnMWaxJvMrY"],"mount_file_id":"11bN8X6a-D77oRCOVhtdIr-5JO3jIlSeB","provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('NLP')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"b0a543fbefe1b64a30a59cd94dbed03a5d22a6e50316e13e19f9b4686843cd44"}}},"nbformat":4,"nbformat_minor":0}
