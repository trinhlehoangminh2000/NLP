{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3132,"status":"ok","timestamp":1665946365772,"user":{"displayName":"Lê Hoàng Minh Trịnh","userId":"12567939653044008206"},"user_tz":-120},"id":"824FC_aU9HRV","outputId":"e0aa417c-47dd-4a50-894c-f0fa6dd2454b"},"outputs":[],"source":["import csv\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","\n","# packages from torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import re\n","\n","from io import StringIO\n","import requests\n","\n","from textblob import TextBlob"]},{"cell_type":"markdown","metadata":{"id":"UFaz2KrIdDgA"},"source":["# Load Data\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"q0gVzd0_9rlf"},"outputs":[],"source":["# Function to load data from scifi.txt\n","def load_scifi(url):\n","    with open(url) as f:\n","        lines = f.readlines()\n","    text = lines[0]\n","    return text \n","\n","# Function to load data from trip advisor\n","def load_tripadvisor(url):\n","    df = pd.read_csv(url)\n","    df.columns = ['Review','Rating']\n","    text = ''\n","    for review in df['Review']:\n","        text+=review\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["Depending on where you run the jupyter notebook, please consider changing the url. Since we run locally, we have the files in data folder (same directory as the notebook)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# replace your url here\n","scifi_url ='./dataset/scifi.txt'\n","tripadvisor_url = './dataset/tripadvisor_hotel_reviews.csv'"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["trip_text = load_tripadvisor(tripadvisor_url)\n","scifi_text = load_scifi(scifi_url)"]},{"cell_type":"markdown","metadata":{"id":"hvnfXuPxA-cC"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["### Todo \n","- Sentence segmentation. (Split sentences by full stop \".\"). \n","- Remove HTML tags\n","- Remove URLs (if needed)\n","- Lower case\n","- Correct spelling\n","- Emojies (if needed)\n","- Chat word treatment (if needed)\n","- Remove special characters and punctuation\n","- Remove rare words because we dont have enough statistic (how infrequent does a word appear to be considered rare?)\n","- Things that DONT HAVE to implement:\n","    - Remove stopword? (not needed if we choose a large window size). Because stop words are frequent and does not have much meaning so think about increasing the window size so that it is not so relevant anymore. If we choose a smaller window size, it makes sense to remove predicting context using stop word as the target word in CBOW)\n","    - Stemming is not needed."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Sentence segmentation \n","def sentence_segmentation(text):\n","    pass\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["class TextCleaner:\n","    # text is an array of string. Each element is a sentence.\n","    def __init__(self,text):\n","        self.text = text\n","    \n","    # Add function call here to choose which cleaning function is used \n","    def get_clean_text(self):\n","        self.remove_html_tags()\n","        self.remove_url()\n","        self.convert_lowercase()\n","        self.correct_spelling()\n","        self.remove_emojies()\n","        self.chatword_treatment()\n","        self.remove_spec()\n","        self.remove_rare_word()\n","\n","        return self.text\n","    \n","    # Remove html tags\n","    def remove_html_tags(self):\n","        re_html = re.compile('<.*?>')\n","        self.text = re_html.sub(r'', self.text)\n","\n","    # Remove url\n","    def remove_url(self):\n","        pass\n","\n","    # Correct spelling function\n","    def correct_spelling(self):\n","        textblob_ = TextBlob(text)\n","        return textblob_.correct().string\n","\n","    # Remove emojies\n","    def remove_emojies(self):\n","        pass\n","\n","    # Lowercase convert\n","    def convert_lowercase(self):\n","        text = text.str.lower()\n","        return text\n","    \n","    # Chat word treatment (abbreviation)\n","    def chatword_treatment(self):\n","        pass\n","\n","    # Remove special characters and punctuation\n","    def remove_spec_char(self):\n","        pass\n","\n","    # Remove rare words\n","    def remove_rare_words(self):\n","        pass\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scifi_cleaner = TextCleaner(scifi_text)\n","scifi = scifi_cleaner.get_clean_text()"]},{"cell_type":"markdown","metadata":{},"source":["# Build CBOW model"]},{"cell_type":"markdown","metadata":{},"source":["Create class CBOW. Take parameters as vocab size and embedding dimensions as input to contruct layers"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["class CBOW(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","        \n","        # Out (context_size*2) x embedding_dimensions\n","        # But will take sum in forward \n","        # => Out 1 x embedding_dimensions\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # Out 1 x 128\n","        self.linear1 = nn.Linear(embedding_dim, 128)\n","        \n","        # Out 1 x 128\n","        # Activation function Relu\n","        self.act1 = nn.ReLU()\n","\n","        # Out 1 x vocab_size\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","        # Out 1 x vocab_size\n","        self.act2 = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, inputs):\n","        # Start of layer 1\n","        l1 = self.embeddings(inputs)\n","        # l1 is of shape (context_size*2) x embedding_dimensions\n","        # sum them up an transform them to 1 x embedding_dimension\n","        l1 = sum(l1).view(1,-1)\n","        l1 = self.linear1(l1)\n","        l1 = self.act1(l1)\n","        \n","        # Start of layer 2, probability is the output of log softmax\n","        l2 = self.linear2(l1)\n","        prob = self.act2(l2)\n","        return prob\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)"]},{"cell_type":"markdown","metadata":{},"source":["Define vocab, etc."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["vocab = set()\n","vocab_size = ...\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","data = []\n","for i in range(2, len(raw_text) - 2):\n","    context = [raw_text[i - 2], raw_text[i - 1],\n","               raw_text[i + 1], raw_text[i + 2]]\n","    target = raw_text[i]\n","    data.append((context, target))"]},{"cell_type":"markdown","metadata":{},"source":["Define parmeters, network and loss function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right\n","EMBEDDING_DIM = 300 # As recommended in the lecture\n","\n","# Set device to cuda to run on GPU\n","device = torch.device(\"cuda\")\n","\n","# choose NLLLoss we want to minize the negative log of softmax\n","loss_function = nn.NLLLoss()\n","losses = []\n","\n","# Set up model\n","model = CBOW(vocab_size, EMBEDDING_DIM).to(device)\n","\n","# Choose optimizer as SGD\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["Trainning loop"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["for epoch in range(100):\n","    total_loss=0\n","    for context, target in data:\n","        # Vector representaion of the context words (by its index from vocab)\n","        # Send to GPU\n","        context_vector = make_context_vector(context, word_to_ix).to(device)\n","\n","        # Calculate output of the model \n","        log_probs = model(context_vector)\n","\n","        # Target vector based on the dataset \n","        # Send to GPU\n","        target_vector = torch.tensor([word_to_ix[target]], dtype=torch.long).to(device)\n","\n","        # Loss of log negative softmax\n","        loss = loss_function(log_probs,target_vector)\n","\n","        # Back propagation and update weight\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss+=loss.item()\n","    losses.append(total_loss)"]},{"cell_type":"markdown","metadata":{},"source":["# Result\n","\n","Nearest neighbor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_closest_word(word, topn=5):\n","    word_distance = []\n","    emb = net.embeddings_target\n","    pdist = nn.PairwiseDistance()\n","    i = word_to_index[word]\n","    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n","    v_i = emb(lookup_tensor_i)\n","    for j in range(len(vocabulary)):\n","        if j != i:\n","            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n","            v_j = emb(lookup_tensor_j)\n","            2\n","            word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n","    word_distance.sort(key=lambda x: x[1])\n","    return word_distance[:topn]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOf+J+HVMCcRAv+X14MdZh3","collapsed_sections":["UFaz2KrIdDgA","jy0KBRDydHYE","BIgN-4nqMyjB","elvPlKkCdRiO","uLnMWaxJvMrY"],"mount_file_id":"11bN8X6a-D77oRCOVhtdIr-5JO3jIlSeB","provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('NLP')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"b0a543fbefe1b64a30a59cd94dbed03a5d22a6e50316e13e19f9b4686843cd44"}}},"nbformat":4,"nbformat_minor":0}
